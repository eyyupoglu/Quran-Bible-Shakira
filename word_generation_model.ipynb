{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "drop_connect.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "Tz-wMAjAMgNC",
        "x_JI08WYM0RR",
        "wN8SklDxYUks",
        "Wrv1_sywkM7Q",
        "otR-ZoEYNk30",
        "KVC3UtTG2izx"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eyyupoglu/Quran-Bible-Shakira/blob/master/word_generation_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "Tz-wMAjAMgNC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Setup the Environment"
      ]
    },
    {
      "metadata": {
        "id": "ziq3yDmSWFiX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# http://pytorch.org/\n",
        "from os.path import exists\n",
        "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
        "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
        "cuda_output = !ldconfig -p|grep cudart.so|sed -e 's/.*\\.\\([0-9]*\\)\\.\\([0-9]*\\)$/cu\\1\\2/'\n",
        "accelerator = cuda_output[0] if exists('/dev/nvidia0') else 'cpu'\n",
        "\n",
        "!pip install -q http://download.pytorch.org/whl/{accelerator}/torch-0.4.1-{platform}-linux_x86_64.whl torchvision\n",
        "import torch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cBuyGcL4WL0K",
        "colab_type": "code",
        "outputId": "b472df93-3f39-4e8e-c446-85b476257076",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "x_JI08WYM0RR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Imports and Constants"
      ]
    },
    {
      "metadata": {
        "id": "YfPiWOy2WL06",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import time\n",
        "import math\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.onnx\n",
        "import numpy as np\n",
        "import pickle\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JKPFOMhCND4Y",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# HYPER PARAMETERS GO HERE\n",
        " \n",
        "emsize = 400\n",
        "batch_size = 40\n",
        "bptt = 40\n",
        "eval_batch_size = 10\n",
        "vocabulary_size = 8000 # is this still needed?\n",
        "nhid = 1150\n",
        "nlayers = 3\n",
        "dropout = 0.5\n",
        "wdrop = 0.5\n",
        "tied = False\n",
        "clip = 0.25\n",
        "log_interval = 100\n",
        "epochs = 10\n",
        "lr = 20\n",
        "\n",
        "\n",
        "model_type = 'LSTM'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MiCt_k1pNF0n",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Tools-Classes"
      ]
    },
    {
      "metadata": {
        "id": "2GPABueUNz7u",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Tools"
      ]
    },
    {
      "metadata": {
        "id": "wN8SklDxYUks",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### DropConnect"
      ]
    },
    {
      "metadata": {
        "id": "GO3gJhEEYb44",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.nn import Parameter\n",
        "from functools import wraps\n",
        "\n",
        "class WeightDrop(torch.nn.Module):\n",
        "    def __init__(self, module, weights, dropout=0, variational=False):\n",
        "        super(WeightDrop, self).__init__()\n",
        "        self.module = module\n",
        "        self.weights = weights\n",
        "        self.dropout = dropout\n",
        "        self.variational = variational\n",
        "        self._setup()\n",
        "\n",
        "    def widget_demagnetizer_y2k_edition(*args, **kwargs):\n",
        "        # We need to replace flatten_parameters with a nothing function\n",
        "        # It must be a function rather than a lambda as otherwise pickling explodes\n",
        "        # We can't write boring code though, so ... WIDGET DEMAGNETIZER Y2K EDITION!\n",
        "        # (╯°□°）╯︵ ┻━┻\n",
        "        return\n",
        "\n",
        "    def _setup(self):\n",
        "        # Terrible temporary solution to an issue regarding compacting weights re: CUDNN RNN\n",
        "        if issubclass(type(self.module), torch.nn.RNNBase):\n",
        "            self.module.flatten_parameters = self.widget_demagnetizer_y2k_edition\n",
        "\n",
        "        for name_w in self.weights:\n",
        "            print('Applying weight drop of {} to {}'.format(self.dropout, name_w))\n",
        "            w = getattr(self.module, name_w)\n",
        "            del self.module._parameters[name_w]\n",
        "            self.module.register_parameter(name_w + '_raw', Parameter(w.data))\n",
        "\n",
        "    def _setweights(self):\n",
        "        for name_w in self.weights:\n",
        "            raw_w = getattr(self.module, name_w + '_raw')\n",
        "            w = None\n",
        "            if self.variational:\n",
        "                mask = torch.autograd.Variable(torch.ones(raw_w.size(0), 1))\n",
        "                if raw_w.is_cuda: mask = mask.cuda()\n",
        "                mask = torch.nn.functional.dropout(mask, p=self.dropout, training=True)\n",
        "                w = mask.expand_as(raw_w) * raw_w\n",
        "            else:\n",
        "                w = torch.nn.functional.dropout(raw_w, p=self.dropout, training=self.training)\n",
        "            setattr(self.module, name_w, w)\n",
        "\n",
        "    def forward(self, *args):\n",
        "        self._setweights()\n",
        "        return self.module.forward(*args)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Wrv1_sywkM7Q",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Our DropConnect"
      ]
    },
    {
      "metadata": {
        "id": "_FjskTi4YgEn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Dropout(nn.Module): \n",
        "    def __init__(self, p=0.5, inplace=False):\n",
        "        super(Dropout, self).__init__()\n",
        "        self.inplace = inplace\n",
        "        self.p = p\n",
        "      \n",
        "    def forward(self, input):  \n",
        "        return F.dropout(input, self.p, self.training, self.inplace)\n",
        "\n",
        "    def __call__(self, x):\n",
        "        return self.forward(x)\n",
        "\n",
        "class DropConnect(nn.Module):\n",
        "    def __init__(self, layer, prob=0.5, **kwargs):\n",
        "        super(DropConnect, self).__init__()\n",
        "        self.prob = prob\n",
        "        self.layer = layer\n",
        "\n",
        "    def __call__(self, x):\n",
        "        if 0. < self.prob < 1.:\n",
        "            self.layer.weight = nn.Parameter(F.dropout(self.layer.weight, self.prob, self.training))\n",
        "        return self.layer(x)\n",
        "      \n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mWbl7-jaSD5b",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Dictionary Corpus"
      ]
    },
    {
      "metadata": {
        "id": "PgPlH_vEWNA0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "\n",
        "class Dictionary(object):\n",
        "    def __init__(self):\n",
        "        self.word2idx = {}\n",
        "        self.idx2word = []\n",
        "\n",
        "    def add_word(self, word):\n",
        "        if word not in self.word2idx:\n",
        "            self.idx2word.append(word)\n",
        "            self.word2idx[word] = len(self.idx2word) - 1\n",
        "        return self.word2idx[word]\n",
        "      \n",
        "    def lookup_word(self, idx):\n",
        "        return self.idx2word[idx]\n",
        "\n",
        "    def lookup_sequence(self, idx_list):\n",
        "        result = []\n",
        "        for i in idx_list:\n",
        "          result.append(self.idx2word[i])\n",
        "        return result\n",
        "      \n",
        "    @staticmethod\n",
        "    def predict(some_string, test_model, corpus, hidden):\n",
        "        \"\"\"\n",
        "        Example usage:\n",
        "        --------------\n",
        "        >>> corpus = Corpus('./gdrive/My Drive/nlp/data/raw/penn-treebank')\n",
        "        >>> file_path = './gdrive/My Drive/nlp/model_wisse1'\n",
        "        >>> test_model = torch.load(file_path).to(device)\n",
        "        >>> corpus = Corpus('./gdrive/My Drive/nlp/data/raw/penn-treebank')\n",
        "        >>> Dictionary.predict('new york stock', test_model = test_model, corpus=corpus)\n",
        "        'exchange'\n",
        "        >>> Dictionary.predict('las ', test_model = test_model, corpus=corpus)\n",
        "        'vegas'\n",
        "        \"\"\"\n",
        "        tokens = corpus.tokenize_string(some_string)\n",
        "        tokens = tokens.unsqueeze(1)\n",
        "        output, hidden = test_model.forward(tokens, hidden)\n",
        "        #excluding <eos>, <unk>, N in our prediction(indices are  24, 26, 27)\n",
        "        output[:,:,24] = 0\n",
        "        output[:,:,26] = 0\n",
        "        output[:,:,27] = 0\n",
        "        _, indices = torch.max(output, 2)       \n",
        "        indices = indices.data.cpu().numpy()\n",
        "        word_list = corpus.dictionary.lookup_sequence(indices[:,0])\n",
        "        output_words = \" \".join(word_list)\n",
        "        return word_list[-1], hidden\n",
        "    \n",
        "    @staticmethod\n",
        "    def conditional_predict(some_word, test_model, corpus, prd_length):\n",
        "      \"\"\"\n",
        "      Example usage:\n",
        "        --------------\n",
        "        >>> corpus = Corpus('./gdrive/My Drive/nlp/data/raw/penn-treebank')\n",
        "        >>> file_path = './gdrive/My Drive/nlp/model_wisse1'\n",
        "        >>> test_model = torch.load(file_path).to(device)\n",
        "        >>> corpus = Corpus('./gdrive/My Drive/nlp/data/raw/penn-treebank')\n",
        "        >>> Dictionary.conditional_predict('francisco', test_model, corpus, 50)\n",
        "        earthquake and the first boston 's recent years ago the company in the company in the first boston 's recent months in the company in the first boston 's recent years ago the company in the company in the company in the company officials in the company officials in the\"\n",
        "      \"\"\"\n",
        "      hidden = test_model.init_hidden(1)\n",
        "      generated_sq = [some_word]\n",
        "      for i in range(prd_length):\n",
        "        some_word, hidden = Dictionary.predict(\" \".join(generated_sq), test_model, corpus, hidden)\n",
        "        generated_sq.append(some_word)\n",
        "      return \" \".join(generated_sq)\n",
        "\n",
        "      \n",
        "    def __len__(self):\n",
        "        return len(self.idx2word)\n",
        "\n",
        "\n",
        "class Corpus(object):\n",
        "    def __init__(self, path):\n",
        "        self.dictionary = Dictionary()\n",
        "        self.train = self.tokenize(os.path.join(path, 'quran_train.txt'))\n",
        "        self.valid = self.tokenize(os.path.join(path, 'quran_val.txt'))\n",
        "        self.test = self.tokenize(os.path.join(path, 'quran_test.txt'))\n",
        "\n",
        "    def tokenize_string(self, s):\n",
        "        \"\"\"Tokenizes a text file.\"\"\"\n",
        "        \n",
        "        tokens = s.split()\n",
        "        # Tokenize file content\n",
        "        ids = torch.LongTensor(len(tokens))\n",
        "        for i, word in enumerate(tokens):\n",
        "            ids[i] = self.dictionary.word2idx[word]\n",
        "\n",
        "        return ids\n",
        "    \n",
        "    def tokenize(self, path):\n",
        "        \"\"\"Tokenizes a text file.\"\"\"\n",
        "        # Add words to the dictionary\n",
        "        with open(path, 'r', encoding=\"utf8\") as f:\n",
        "\n",
        "            tokens = 0\n",
        "            for line in f:\n",
        "                words = line.split() + ['<eos>']\n",
        "                tokens += len(words)\n",
        "                for word in words:\n",
        "                    self.dictionary.add_word(word)\n",
        "\n",
        "        # Tokenize file content\n",
        "        with open(path, 'r', encoding=\"utf8\") as f:\n",
        "            ids = torch.LongTensor(tokens)\n",
        "            token = 0\n",
        "            for line in f:\n",
        "                words = line.split() + ['<eos>']\n",
        "                for word in words:\n",
        "                    ids[token] = self.dictionary.word2idx[word]\n",
        "                    token += 1\n",
        "\n",
        "        return ids"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GRK7gjdIYQs8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "###############################################################################\n",
        "# Load data\n",
        "###############################################################################\n",
        "corpus = Corpus('./gdrive/My Drive/nlp/data/raw/penn-treebank')\n",
        "\n",
        "# Starting from sequential data, batchify arranges the dataset into columns.\n",
        "# For instance, with the alphabet as the sequence and batch size 4, we'd get\n",
        "# ┌ a g m s ┐\n",
        "# │ b h n t │\n",
        "# │ c i o u │\n",
        "# │ d j p v │\n",
        "# │ e k q w │\n",
        "# └ f l r x ┘.\n",
        "# These columns are treated as independent by the model, which means that the\n",
        "# dependence of e. g. 'g' on 'f' can not be learned, but allows more efficient\n",
        "# batch processing.\n",
        "\n",
        "def batchify(data, bsz):\n",
        "    # Work out how cleanly we can divide the dataset into bsz parts.\n",
        "    nbatch = data.size(0) // bsz\n",
        "    # Trim off any extra elements that wouldn't cleanly fit (remainders).\n",
        "    data = data.narrow(0, 0, nbatch * bsz)\n",
        "    # Evenly divide the data across the bsz batches.\n",
        "    data = data.view(bsz, -1).t().contiguous()\n",
        "    return data.to(device)\n",
        "\n",
        "train_data = batchify(corpus.train, batch_size)\n",
        "val_data = batchify(corpus.valid, eval_batch_size)\n",
        "test_data = batchify(corpus.test, eval_batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "J3XH357ESwI9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def repackage_hidden(h):\n",
        "    \"\"\"Wraps hidden states in new Tensors, to detach them from their history.\"\"\"\n",
        "#     print('repackage hiddem', type(hidden))\n",
        "    if isinstance(h, torch.Tensor):\n",
        "      return h.detach()\n",
        "    else:\n",
        "        return tuple(repackage_hidden(v) for v in h)\n",
        "\n",
        "\n",
        "def get_batch(source, i):\n",
        "    seq_len = min(bptt, len(source) - 1 - i)\n",
        "    data = source[i:i+seq_len]\n",
        "    target = source[i+1:i+1+seq_len].view(-1)\n",
        "    return data, target\n",
        "\n",
        "\n",
        "def evaluate(data_source):\n",
        "    # Turn on evaluation mode which disables dropout.\n",
        "    model.eval()\n",
        "    total_loss = 0.\n",
        "    ntokens = len(corpus.dictionary)\n",
        "    if model_type == 'LSTM':\n",
        "      hidden = model.init_hidden(eval_batch_size)\n",
        "    else:\n",
        "      hidden, _ = model.init_hidden(eval_batch_size)\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, data_source.size(0) - 1, bptt):\n",
        "            data, targets = get_batch(data_source, i)\n",
        "            output, hidden = model(data, hidden)\n",
        "            output_flat = output.view(-1, ntokens)\n",
        "            total_loss += len(data) * criterion(output_flat, targets).item()\n",
        "            hidden = repackage_hidden(hidden)\n",
        "    return total_loss / (len(data_source) - 1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_7Bvl-XbfkY0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Utility module to deal with lists of layers, see https://discuss.pytorch.org/t/list-of-nn-module-in-a-nn-module/219/2\n",
        "class ListModule(nn.Module):\n",
        "    def __init__(self, *args):\n",
        "        super(ListModule, self).__init__()\n",
        "        idx = 0\n",
        "        for module in args:\n",
        "            self.add_module(str(idx), module)\n",
        "            idx += 1\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if idx < 0 or idx >= len(self._modules):\n",
        "            raise IndexError('index {} is out of range'.format(idx))\n",
        "        it = iter(self._modules.values())\n",
        "        for i in range(idx):\n",
        "            next(it)\n",
        "        return next(it)\n",
        "\n",
        "    def __iter__(self):\n",
        "        return iter(self._modules.values())\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self._modules)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "otR-ZoEYNk30",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Models"
      ]
    },
    {
      "metadata": {
        "id": "6sFFwhPcTxAN",
        "colab_type": "code",
        "outputId": "6136bb70-1857-4618-8db3-cfadf4875057",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "# coding: utf-8\n",
        "\n",
        "# In[1]:\n",
        "\n",
        "import itertools\n",
        "import operator\n",
        "from datetime import datetime\n",
        "import sys\n",
        "from torch import FloatTensor\n",
        "from torch.autograd import Variable\n",
        "from torch import nn\n",
        "\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class RNN_mehmet(nn.Module):\n",
        "    def __init__(self, word_dim, hidden_dim = 100, activation = 'sigmoid'):\n",
        "        super(RNN_mehmet, self).__init__()\n",
        "\n",
        "        self.word_dim = word_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.weights_hh = self.init_weights((hidden_dim, hidden_dim))\n",
        "        self.weights_xh = self.init_weights((hidden_dim, word_dim))\n",
        "        self.weights_o = self.init_weights((word_dim, hidden_dim))\n",
        "        self.activation = getattr(torch, activation)\n",
        "\n",
        "\n",
        "  \n",
        "    def init_weights(self, dim):\n",
        "        return nn.Parameter(torch.FloatTensor(dim[0], dim[1]).uniform_(-np.sqrt(1./dim[0]), np.sqrt(1./dim[1])), requires_grad=True)\n",
        "      \n",
        "    \n",
        "    def init_hidden(self, batch_size, dim):\n",
        "      \n",
        "        layer = torch.zeros((1, batch_size, self.hidden_dim),  requires_grad = True)\n",
        "        if dim > 1:\n",
        "           layer = (layer.clone(), layer.clone())\n",
        "        return layer\n",
        "      \n",
        "    def step(self, lr):\n",
        "        for p in self.parameters():\n",
        "            p.data.add_(-lr, p.grad.data)\n",
        "      \n",
        "    def forward_step(self, xt, hidden_t_1):\n",
        "        \n",
        "        # calculate left and right terms\n",
        "        left_term = F.linear(xt, self.weights_xh)\n",
        "        right_term = F.linear(hidden_t_1, self.weights_hh)\n",
        "        \n",
        "        # sum terms\n",
        "        sum_ = left_term + right_term\n",
        "        \n",
        "        # activation for hidden state\n",
        "        hidden_t = self.activation(sum_)\n",
        "        \n",
        "        # calculate output\n",
        "        output = F.linear(hidden_t, self.weights_o)\n",
        "        return output, hidden_t\n",
        "\n",
        "    def forward_propagation(self, x, hidden_t_1):\n",
        "        # Get sequence length (bptt), batch_size from the input\n",
        "        bptt, batch_size, _ = x.size()\n",
        "        output = torch.zeros((bptt, batch_size, self.word_dim)).to(device)\n",
        "        \n",
        "        # loop over sequence\n",
        "        for t in torch.arange(bptt): \n",
        "            xt = x[t,:,:]\n",
        "            output[t], hidden_t_1 = self.forward_step(xt, hidden_t_1)\n",
        "        return [output, hidden_t_1]\n",
        "      \n",
        "    def __call__(self, x, hidden_t_1):\n",
        "        return self.forward_propagation(x, hidden_t_1)\n",
        "      \n",
        "\n",
        "fake_net = RNN_mehmet(emsize, hidden_dim = 130)\n",
        "\n",
        "#forward_prop\n",
        "# test forward pass\n",
        "x = np.random.normal(0, 1, (bptt, batch_size, emsize)).astype('float32')\n",
        "x = torch.Tensor(torch.from_numpy(x))\n",
        "\n",
        "hidden = fake_net.init_hidden(batch_size, 1)\n",
        "\n",
        "y = np.random.normal(0, 1, (bptt * batch_size, emsize)).astype('float32')\n",
        "y = torch.Tensor(torch.from_numpy(y)).long().to(device)\n",
        "\n",
        "\n",
        "#example backward and 10 steps\n",
        "\n",
        "for i in range(10):\n",
        "    fake_net.zero_grad()\n",
        "    result = fake_net.forward_propagation(x, hidden)\n",
        "    output =  result[0].view(-1, emsize)\n",
        "\n",
        "    loss_fn = nn.MSELoss()\n",
        "    loss = loss_fn(output, y.float())\n",
        "    print(loss)\n",
        "    loss.backward()\n",
        "    fake_net.step(0.01)\n",
        "\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(2.0390, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "tensor(2.0299, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "tensor(2.0208, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "tensor(2.0119, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "tensor(2.0030, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "tensor(1.9943, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "tensor(1.9856, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "tensor(1.9770, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "tensor(1.9686, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "tensor(1.9602, device='cuda:0', grad_fn=<MseLossBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "4aCeHkKQfVG9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "# coding: utf-8\n",
        "\n",
        "# In[1]:\n",
        "\n",
        "import itertools\n",
        "import operator\n",
        "from datetime import datetime\n",
        "import sys\n",
        "from torch import FloatTensor\n",
        "from torch.autograd import Variable\n",
        "from torch import nn\n",
        "\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class LSTMCustom(nn.Module):\n",
        "    def __init__(self, word_dim, hidden_dim, nlayers = 1, activation = 'sigmoid'):\n",
        "        super(LSTMCustom, self).__init__()\n",
        "\n",
        "        self.word_dim = word_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "   \n",
        "        # weights for x\n",
        "        self.weights_xi = self.init_weights((hidden_dim, word_dim))\n",
        "        self.weights_xo = self.init_weights((hidden_dim, word_dim))\n",
        "        self.weights_xf = self.init_weights((hidden_dim, word_dim))\n",
        "        self.weights_xc = self.init_weights((hidden_dim, word_dim))\n",
        "\n",
        "        # weights for hidden\n",
        "        self.weights_hi = self.init_weights((hidden_dim, hidden_dim))\n",
        "        self.weights_ho = self.init_weights((hidden_dim, hidden_dim))\n",
        "        self.weights_hf = self.init_weights((hidden_dim, hidden_dim))\n",
        "        self.weights_hc = self.init_weights((hidden_dim, hidden_dim))\n",
        "        \n",
        "        self.weights_h_out = self.init_weights((hidden_dim, hidden_dim))\n",
        "        \n",
        "\n",
        "        \n",
        "  \n",
        "    def init_weights(self, dim):\n",
        "        return nn.Parameter(torch.FloatTensor(dim[0], dim[1]).uniform_(-np.sqrt(1./dim[0]), np.sqrt(1./dim[1])), requires_grad=True)\n",
        "  \n",
        "    def init_gates(self):\n",
        "        return torch.zeros(self.hidden_dim,  requires_grad = True)\n",
        "    \n",
        "    def init_hidden(self, batch_size):\n",
        "        layer = torch.zeros((1, batch_size, self.hidden_dim),  requires_grad = True)\n",
        "        return (layer.clone(), layer.clone())\n",
        "      \n",
        "    def step(self, lr):\n",
        "        for p in self.parameters():\n",
        "            p.data.add_(-lr, p.grad.data)\n",
        "      \n",
        "    def forward_step(self, xt, hidden_t_1):\n",
        "        \n",
        "        ht_1, ct_1 = hidden_t_1\n",
        "        gate_i = torch.sigmoid(\n",
        "            F.linear(xt, self.weights_xi) + \n",
        "            F.linear(ht_1, self.weights_hi))\n",
        "        \n",
        "        gate_f = torch.sigmoid(\n",
        "            F.linear(xt, self.weights_xf) + \n",
        "            F.linear(ht_1, self.weights_hf))\n",
        "        \n",
        "        gate_o = torch.sigmoid(\n",
        "            F.linear(xt, self.weights_xo) + \n",
        "            F.linear(ht_1, self.weights_ho))\n",
        "        \n",
        "        new_c = torch.tanh(\n",
        "            F.linear(xt, self.weights_xc) +\n",
        "            F.linear(ht_1, self.weights_hc))\n",
        "        \n",
        "        ct = gate_f * ct_1 + gate_i * new_c\n",
        "        \n",
        "        ht = gate_o * torch.tanh(ct)\n",
        "        \n",
        "        output = F.linear(ht, self.weights_h_out)\n",
        "        \n",
        "        return output, (ht, ct)\n",
        "\n",
        "    def forward(self, x, hidden_t_1):\n",
        "        # Get sequence length (bptt), batch_size from the input\n",
        "        bptt, batch_size, _ = x.size()\n",
        "        output = torch.zeros((bptt, batch_size, self.hidden_dim)).to(device)\n",
        "        \n",
        "        # loop over sequence\n",
        "        for t in torch.arange(bptt):\n",
        "            xt = x[t,:,:]\n",
        "            output[t], hidden_t_1 = self.forward_step(xt, hidden_t_1)\n",
        "            \n",
        "        return [output, hidden_t_1]\n",
        "      \n",
        "    def __call__(self, x, hidden_t_1):\n",
        "        return self.forward_propagation(x, hidden_t_1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wh7SoG93ZtLe",
        "colab_type": "code",
        "outputId": "784c8b66-16f1-4f0a-d0b9-9147a20a3397",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 525
        }
      },
      "cell_type": "code",
      "source": [
        "# New RNN Layer\n",
        "\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class RNNCustom(nn.Module):\n",
        "  \n",
        "  def __init__(self, ninput, nhid, activation = 'sigmoid'):\n",
        "      self.ninput = ninput\n",
        "      self.nhid = nhid\n",
        "      self.weights_hh = self.init_weights((nhid, nhid))\n",
        "      self.weights_xh = self.init_weights((nhid, ninput))\n",
        "      self.activation = getattr(F, activation)\n",
        "      self._modules = {}\n",
        "    \n",
        "  def init_weights(self, dimensions):\n",
        "      return torch.zeros(dimensions, requires_grad = True).to(device)\n",
        "      \n",
        "  def init_hidden(self):\n",
        "      layer = torch.zeros((1, batch_size, self.nhid),  requires_grad = True).to(device)\n",
        "      return (layer, layer)\n",
        "    \n",
        "  def step(self, xt, ht_1):\n",
        "      # calculate product of weights and inputs\n",
        "      xt = F.linear(xt, self.weights_xh)\n",
        "      ht_1 = F.linear(ht_1, self.weights_hh)\n",
        "      \n",
        "      # return activation of concatenated products\n",
        "      return self.activation(xt + ht_1), ht_1\n",
        "      \n",
        "    \n",
        "  def forward(self, x, hidden):\n",
        "      # Get sequence length (bptt), batch_size from the input\n",
        "      bptt, batch_size, _ = x.size()\n",
        "      \n",
        "      # intialize output\n",
        "      output = torch.zeros((bptt, batch_size, self.ninput), requires_grad = True).to(device)\n",
        "      \n",
        "      # hidden layers\n",
        "      ht_1, ht = hidden\n",
        "      \n",
        "      # loop over sequence\n",
        "      for i in range(bptt):\n",
        "        \n",
        "        # slice input \n",
        "        xt = x[i,:,:]\n",
        "       \n",
        "        # store step output\n",
        "        output[i,:,:], ht = self.step(xt, ht_1)\n",
        "        \n",
        "        # update hidden states\n",
        "        ht_1 = ht\n",
        "      \n",
        "      # return output, (hidden, hidden)\n",
        "      return output, (ht_1, ht)\n",
        "    \n",
        "  def __call__(self, x, hidden):\n",
        "      return self.forward(x, hidden)\n",
        "    \n",
        "\n",
        "# test forward pass\n",
        "x = np.random.normal(0, 1, (bptt, batch_size, emsize)).astype('float32')\n",
        "\n",
        "# double hidden only necesary for LSTM\n",
        "hidden_layer = np.zeros((1, batch_size, nhid)).astype('float32')\n",
        "hidden_layer_tensor = torch.Tensor(torch.from_numpy(hidden_layer)).to(device)\n",
        "hidden = (hidden_layer_tensor, hidden_layer_tensor)\n",
        "\n",
        "\n",
        "# output is still very different\n",
        "rnn_pt = nn.RNN(emsize, nhid)\n",
        "output, h = rnn_pt(torch.Tensor(torch.from_numpy(x)))\n",
        "# print('RNN TORCH', output)\n",
        "\n",
        "rnn_cust = RNNCustom(ninput = emsize, nhid = nhid)\n",
        "output, h = rnn_cust(torch.Tensor(torch.from_numpy(x)).to(device), hidden)\n",
        "# print('RNN cust', output)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1006: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-cd0b32c91858>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0mrnn_cust\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRNNCustom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mninput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0memsize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnhid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnhid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrnn_cust\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;31m# print('RNN cust', output)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-cd0b32c91858>\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, x, hidden)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-cd0b32c91858>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, hidden)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;31m# store step output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mht\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mht_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;31m# update hidden states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: The expanded size of the tensor (400) must match the existing size (1150) at non-singleton dimension 1"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "KVC3UtTG2izx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## RNNModel"
      ]
    },
    {
      "metadata": {
        "id": "7FblJYsLY-hL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "class RNNModel(nn.Module):\n",
        "    \"\"\"Container module with an encoder, a recurrent module, and a decoder.\"\"\"\n",
        "\n",
        "    def __init__(self, rnn_type, ntoken, ninp, nhid, nlayers, dropout=0.5,wdrop = 0.6, tie_weights=False):\n",
        "        super(RNNModel, self).__init__()\n",
        "        self.drop = Dropout(dropout)#our dropout\n",
        "#         self.drop = nn.Dropout(dropout)\n",
        "        self.encoder = nn.Embedding(ntoken, ninp)\n",
        "        \n",
        "        # quick fix to match up dimensions of internal LSTM layers\n",
        "        # setting all but last LSTM layer to transform [ninp -> nhid], and last one [nhid -> nhid]\n",
        "        LSTM_dimensions = [(ninp, nhid) if i == 0 else (nhid, nhid) for i in range(nlayers)]\n",
        "        self.rnns = [LSTMCustom(*LSTM_dimensions[i]).to(device) for i in range(nlayers)]\n",
        "                \n",
        "        \n",
        "        print('wdrop: ', wdrop)\n",
        "        self.rnns = [WeightDrop(rnn, ['weights_hc'], dropout=wdrop) for rnn in self.rnns]\n",
        "\n",
        "#         if rnn_type in ['LSTM', 'GRU']:\n",
        "#             self.rnn = getattr(nn, rnn_type)(ninp, nhid, nlayers, dropout=dropout)\n",
        "#         else:\n",
        "#             try:\n",
        "#                 nonlinearity = {'RNN_TANH': 'tanh', 'RNN_RELU': 'relu'}[rnn_type]\n",
        "#             except KeyError:\n",
        "#                 raise ValueError( \"\"\"An invalid option for `--model` was supplied,\n",
        "#                                  options are ['LSTM', 'GRU', 'RNN_TANH' or 'RNN_RELU']\"\"\")\n",
        "#             self.rnn = nn.RNN(ninp, nhid, nlayers, nonlinearity=nonlinearity, dropout=dropout)\n",
        "        self.decoder = nn.Linear( nhid, ntoken )\n",
        "#         self.dropconnect = DropConnect(self.decoder, dropout)\n",
        "        # Optionally tie weights as in:\n",
        "        # \"Using the Output Embedding to Improve Language Models\" (Press & Wolf 2016)\n",
        "        # https://arxiv.org/abs/1608.05859\n",
        "        # and\n",
        "        # \"Tying Word Vectors and Word Classifiers: A Loss Framework for Language Modeling\" (Inan et al. 2016)\n",
        "        # https://arxiv.org/abs/1611.01462\n",
        "        if tie_weights:\n",
        "            if nhid != ninp:\n",
        "                raise ValueError('When using the tied flag, nhid must be equal to emsize')\n",
        "            self.decoder.weight = self.encoder.weight\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "        self.rnn_type = rnn_type\n",
        "        self.nhid = nhid\n",
        "        self.nlayers = nlayers\n",
        "        self.rnns = ListModule(*self.rnns)\n",
        "\n",
        "    def init_weights(self):\n",
        "        initrange = 0.1\n",
        "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
        "        self.decoder.bias.data.zero_()\n",
        "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "    def forward(self, x, hidden):\n",
        "        # Send to device explicitly, because the loop over LSTM layers \n",
        "        # in the Net breaks the normal .to(decvice) functionality\n",
        "        x = x.to(device)\n",
        "        hidden = (hidden[0].to(device), hidden[1].to(device))\n",
        "        x = self.drop(self.encoder(x))\n",
        "        for i, rnn in enumerate(self.rnns):\n",
        "          x, hidden = rnn(x, hidden)\n",
        "        x = self.drop(x)\n",
        "        decoded = self.decoder(x.view(x.size(0)*x.size(1), x.size(2)))\n",
        "#         decoded = self.dropconnect( x.view(x.size(0)*x.size(1), x.size(2)) )\n",
        "        ext_output = decoded.view(x.size(0), x.size(1), decoded.size(1))\n",
        "        return ext_output, hidden            \n",
        "\n",
        "\n",
        "    def init_hidden(self, bsz):\n",
        "        init_range = 1/np.sqrt(self.nhid)\n",
        "        weight = next(self.parameters())\n",
        "        if self.rnn_type == 'LSTM':\n",
        "            return (weight.new_zeros(1, bsz, self.nhid).uniform_(-init_range, init_range),\n",
        "                    weight.new_zeros(1, bsz, self.nhid).uniform_(-init_range, init_range))\n",
        "        else:\n",
        "            return weight.new_zeros(self.nlayers, bsz, self.nhid).uniform_(-init_range, init_range)\n",
        "\n",
        "          \n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ea9eMQSePmRZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Training"
      ]
    },
    {
      "metadata": {
        "id": "CIa4B38jZUIN",
        "colab_type": "code",
        "outputId": "48b0bea6-6678-4a3a-eca2-96645244ed5d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 372
        }
      },
      "cell_type": "code",
      "source": [
        "###############################################################################\n",
        "# Build the model\n",
        "###############################################################################\n",
        "ntokens = len(corpus.dictionary)\n",
        "model = RNNModel('LSTM', ntokens, emsize, nhid, nlayers, dropout, wdrop, tied).to(device)\n",
        "model.train()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "print(model)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "wdrop:  0.5\n",
            "Applying weight drop of 0.5 to weights_hc\n",
            "Applying weight drop of 0.5 to weights_hc\n",
            "Applying weight drop of 0.5 to weights_hc\n",
            "RNNModel(\n",
            "  (drop): Dropout()\n",
            "  (encoder): Embedding(5595, 400)\n",
            "  (decoder): Linear(in_features=1150, out_features=5595, bias=True)\n",
            "  (rnns): ListModule(\n",
            "    (0): WeightDrop(\n",
            "      (module): LSTMCustom()\n",
            "    )\n",
            "    (1): WeightDrop(\n",
            "      (module): LSTMCustom()\n",
            "    )\n",
            "    (2): WeightDrop(\n",
            "      (module): LSTMCustom()\n",
            "    )\n",
            "  )\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Jycx92tZZW4D",
        "colab_type": "code",
        "outputId": "fc6ee5ef-f136-444c-905b-f9dc85f89b37",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 570
        }
      },
      "cell_type": "code",
      "source": [
        "###############################################################################\n",
        "# Training code\n",
        "###############################################################################\n",
        "\n",
        "def train():\n",
        "    # Turn on training mode which enables dropout.\n",
        "    model.train()\n",
        "    total_loss = 0.\n",
        "    start_time = time.time()\n",
        "    ntokens = len(corpus.dictionary)\n",
        "    if model_type == 'LSTM':\n",
        "        hidden = model.init_hidden(batch_size)\n",
        "    else:\n",
        "        hidden, _ = model.init_hidden(batch_size)\n",
        "    for batch, i in enumerate(range(0, train_data.size(0), bptt)):\n",
        "        data, targets = get_batch(train_data, i)\n",
        "\n",
        "        # Starting each batch, we detach the hidden state from how it was previously produced.\n",
        "        # If we didn't, the model would try backpropagating all the way to start of the dataset.\n",
        "        hidden = repackage_hidden(hidden)\n",
        "        model.zero_grad()\n",
        "        output, hidden = model(data, hidden)\n",
        "        loss = criterion(output.view(-1, ntokens), targets)\n",
        "        loss.backward()\n",
        "\n",
        "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        for p in model.parameters():\n",
        "            try:\n",
        "              p.data.add_(-lr, p.grad.data)\n",
        "            except Exception as e:\n",
        "              print(e)\n",
        "              pass\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        if batch % log_interval == 0 and batch > 0:\n",
        "            cur_loss = total_loss / log_interval\n",
        "            elapsed = time.time() - start_time\n",
        "            try:\n",
        "              print('| epoch {:3d} | {:5d}/{:5d} batches | lr {:02.2f} | ms/batch {:5.2f} | '\n",
        "                      'loss {:5.2f} | ppl {:8.2f}'.format(\n",
        "                  epoch, batch, len(train_data) // bptt, lr,\n",
        "                  elapsed * 1000 / log_interval, cur_loss, math.exp(cur_loss)))\n",
        "            except:\n",
        "              print('math error the same')\n",
        "            total_loss = 0\n",
        "            start_time = time.time()\n",
        "\n",
        "# Loop over epochs.\n",
        "\n",
        "best_val_loss = None\n",
        "save_file = './gdrive/My Drive/nlp/model_ep{}_nlayer{}_em{}_nhid{}_bptt{}_tied{}_bs{}_embdrop_{}_clip{}_lr{}'.format(epochs, nlayers, emsize, nhid, bptt, tied, batch_size, dropout, clip, lr)\n",
        "meta_file = save_file + '.meta'\n",
        "meta_data = []\n",
        "\n",
        "# At any point you can hit Ctrl + C to break out of training early.\n",
        "try:\n",
        "    for epoch in range(1, epochs+1):\n",
        "        epoch_start_time = time.time()\n",
        "        train()\n",
        "        val_loss = evaluate(val_data)\n",
        "        print('-' * 89)\n",
        "        print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n",
        "                'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n",
        "                                           val_loss, math.exp(val_loss)))\n",
        "        print('-' * 89)\n",
        "        # Save the model if the validation loss is the best we've seen so far.\n",
        "        if not best_val_loss or val_loss < best_val_loss:\n",
        "\n",
        "            with open(save_file, 'wb') as f:\n",
        "                torch.save(model, f)\n",
        "            best_val_loss = val_loss\n",
        "            meta = {\n",
        "                'epoch': epoch,\n",
        "                'time': (time.time() - epoch_start_time),\n",
        "                'val_loss': val_loss,\n",
        "                'val_ppl': math.exp(val_loss),\n",
        "                'lr': lr,\n",
        "            }\n",
        "            meta_data.append(meta)\n",
        "            with open(meta_file, 'wb') as f:\n",
        "                pickle.dump(meta_data, f)\n",
        "            \n",
        "        else:\n",
        "            # Anneal the learning rate if no improvement has been seen in the validation dataset.\n",
        "            lr /= 4.0\n",
        "except KeyboardInterrupt:\n",
        "    valid_file = save_file + '_val_loss{:5.2f}_ppl_{:8.2f}'.format(best_val_loss, math.exp(best_val_loss))\n",
        "    with open(valid_file, 'w') as f:\n",
        "      f.write('-----')\n",
        "    print('-' * 89)\n",
        "    print('Exiting from training early')\n",
        "\n",
        "valid_file = save_file + '_val_loss{:5.2f}_ppl_{:8.2f}'.format(best_val_loss, math.exp(best_val_loss))\n",
        "with open(valid_file, 'w') as f:\n",
        "  f.write('-----')\n",
        "   \n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "| epoch   1 |   100/  102 batches | lr 5.00 | ms/batch 782.67 | loss  4.42 | ppl    82.75\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   1 | time: 80.31s | valid loss  4.37 | valid ppl    78.76\n",
            "-----------------------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type RNNModel. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type Dropout. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type ListModule. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type WeightDrop. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type LSTMCustom. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "| epoch   2 |   100/  102 batches | lr 5.00 | ms/batch 780.79 | loss  4.34 | ppl    76.78\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   2 | time: 80.15s | valid loss  4.32 | valid ppl    74.87\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   3 |   100/  102 batches | lr 5.00 | ms/batch 786.49 | loss  4.27 | ppl    71.17\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   3 | time: 80.73s | valid loss  4.26 | valid ppl    71.08\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   4 |   100/  102 batches | lr 5.00 | ms/batch 788.71 | loss  4.19 | ppl    66.17\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   4 | time: 80.93s | valid loss  4.22 | valid ppl    67.80\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   5 |   100/  102 batches | lr 5.00 | ms/batch 789.88 | loss  4.13 | ppl    61.92\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   5 | time: 81.06s | valid loss  4.17 | valid ppl    64.59\n",
            "-----------------------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "jnIpeTGWSR8y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "aadfbd0d-961f-46f9-a6b7-78a0a95f4ea8"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "Dictionary.conditional_predict('there', model, corpus, 15)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'there polytheists enough the falsehood Solomon the smell Solomon the smell Solomon the smell Solomon the'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "metadata": {
        "id": "CI8gyLKHpAMC",
        "colab_type": "code",
        "outputId": "cb683e93-0c64-448b-a18a-4e23cb632945",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        }
      },
      "cell_type": "code",
      "source": [
        "# Loading model\n",
        "\n",
        "test_model = torch.load(save_file)\n",
        "print(test_model)\n",
        "\n",
        "# load meta\n",
        "with open(save_file + '.meta', 'rb') as f:\n",
        "  meta = pickle.load(f)\n",
        "  print(meta)\n",
        "  \n",
        "# number of parameters (untested)\n",
        "def get_n_params(model):\n",
        "    pp=0\n",
        "    for p in list(model.parameters()):\n",
        "        nn=1\n",
        "        for s in list(p.size()):\n",
        "            nn = nn*s\n",
        "        pp += nn\n",
        "    return pp"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "RNNModel(\n",
            "  (drop): Dropout(p=0.5)\n",
            "  (encoder): Embedding(10000, 400)\n",
            "  (rnns): ListModule(\n",
            "    (0): LSTMCustom()\n",
            "    (1): LSTMCustom()\n",
            "    (2): LSTMCustom()\n",
            "  )\n",
            "  (decoder): Linear(in_features=1150, out_features=10000, bias=True)\n",
            ")\n",
            "[{'epoch': 1, 'time': 500.2183609008789, 'val_loss': 6.094107644355903, 'val_ppl': 443.2383424130543, 'lr': 20}, {'epoch': 2, 'time': 500.0337471961975, 'val_loss': 5.587101821576135, 'val_ppl': 266.960797446221, 'lr': 20}, {'epoch': 3, 'time': 500.3192808628082, 'val_loss': 5.343247472148831, 'val_ppl': 209.1909501949757, 'lr': 20}, {'epoch': 4, 'time': 500.0058431625366, 'val_loss': 5.2122298108117056, 'val_ppl': 183.5027788197245, 'lr': 20}, {'epoch': 5, 'time': 500.48462748527527, 'val_loss': 5.133896831577107, 'val_ppl': 169.67703424361176, 'lr': 20}, {'epoch': 6, 'time': 507.9676570892334, 'val_loss': 4.998721956802627, 'val_ppl': 148.22360183117735, 'lr': 20}, {'epoch': 7, 'time': 500.00479006767273, 'val_loss': 4.900065622168072, 'val_ppl': 134.29859236057865, 'lr': 20}, {'epoch': 8, 'time': 499.478698015213, 'val_loss': 4.742235275850458, 'val_ppl': 114.69027978361741, 'lr': 20}, {'epoch': 9, 'time': 509.7620186805725, 'val_loss': 4.561224732803086, 'val_ppl': 95.7006157687188, 'lr': 20}, {'epoch': 10, 'time': 509.6283121109009, 'val_loss': 4.284614952539994, 'val_ppl': 72.57459667494219, 'lr': 20}, {'epoch': 11, 'time': 505.7668423652649, 'val_loss': 4.177889898267843, 'val_ppl': 65.22807003330686, 'lr': 20}, {'epoch': 12, 'time': 506.82263565063477, 'val_loss': 4.03278673204325, 'val_ppl': 56.4179139919673, 'lr': 20}, {'epoch': 13, 'time': 500.00451731681824, 'val_loss': 3.9610769663018695, 'val_ppl': 52.51385115383934, 'lr': 20}, {'epoch': 14, 'time': 498.7160291671753, 'val_loss': 3.7406678568306617, 'val_ppl': 42.12611498591722, 'lr': 20}, {'epoch': 16, 'time': 498.51683282852173, 'val_loss': 3.002527674012265, 'val_ppl': 20.13637083160002, 'lr': 5.0}, {'epoch': 17, 'time': 505.02328729629517, 'val_loss': 2.9163388024346304, 'val_ppl': 18.473528258840208, 'lr': 5.0}, {'epoch': 18, 'time': 498.8268744945526, 'val_loss': 2.836479994725373, 'val_ppl': 17.055623863778038, 'lr': 5.0}, {'epoch': 19, 'time': 498.50012969970703, 'val_loss': 2.7394870526911848, 'val_ppl': 15.479043126093375, 'lr': 5.0}, {'epoch': 20, 'time': 500.34062099456787, 'val_loss': 2.667154292737023, 'val_ppl': 14.398935679967458, 'lr': 5.0}, {'epoch': 21, 'time': 500.2378647327423, 'val_loss': 2.604548344046383, 'val_ppl': 13.525115224210468, 'lr': 5.0}, {'epoch': 22, 'time': 500.3882474899292, 'val_loss': 2.5750431750992595, 'val_ppl': 13.131884116650891, 'lr': 5.0}, {'epoch': 23, 'time': 500.2405514717102, 'val_loss': 2.487928810604548, 'val_ppl': 12.03632078578186, 'lr': 5.0}, {'epoch': 24, 'time': 500.1563198566437, 'val_loss': 2.453015588016833, 'val_ppl': 11.623345142615582, 'lr': 5.0}, {'epoch': 25, 'time': 497.69585728645325, 'val_loss': 2.3740693871449614, 'val_ppl': 10.741012808962179, 'lr': 5.0}, {'epoch': 27, 'time': 500.0548269748688, 'val_loss': 1.95042462914677, 'val_ppl': 7.0316727999602735, 'lr': 1.25}, {'epoch': 28, 'time': 505.0379831790924, 'val_loss': 1.8970742511749268, 'val_ppl': 6.666361782232105, 'lr': 1.25}, {'epoch': 29, 'time': 504.8403089046478, 'val_loss': 1.8711087834633002, 'val_ppl': 6.4954945048720045, 'lr': 1.25}, {'epoch': 30, 'time': 498.7811861038208, 'val_loss': 1.8376746549444682, 'val_ppl': 6.281913647141092, 'lr': 1.25}, {'epoch': 31, 'time': 499.46734499931335, 'val_loss': 1.8194081862498137, 'val_ppl': 6.168206939802222, 'lr': 1.25}, {'epoch': 32, 'time': 500.00215196609497, 'val_loss': 1.790540973210739, 'val_ppl': 5.9926934762851465, 'lr': 1.25}, {'epoch': 33, 'time': 499.8248209953308, 'val_loss': 1.7731821089275812, 'val_ppl': 5.889564811423431, 'lr': 1.25}, {'epoch': 34, 'time': 499.86474204063416, 'val_loss': 1.7685035047692768, 'val_ppl': 5.862074227931091, 'lr': 1.25}, {'epoch': 35, 'time': 499.6685245037079, 'val_loss': 1.7631053499448097, 'val_ppl': 5.830515100819579, 'lr': 1.25}, {'epoch': 36, 'time': 499.4868109226227, 'val_loss': 1.730762369673131, 'val_ppl': 5.644955811516601, 'lr': 1.25}, {'epoch': 37, 'time': 499.5266869068146, 'val_loss': 1.7256379079010526, 'val_ppl': 5.616102443387421, 'lr': 1.25}, {'epoch': 38, 'time': 499.463383436203, 'val_loss': 1.7206267738342285, 'val_ppl': 5.588029797751446, 'lr': 1.25}, {'epoch': 39, 'time': 499.44397592544556, 'val_loss': 1.7123219853740628, 'val_ppl': 5.541814561870712, 'lr': 1.25}, {'epoch': 40, 'time': 499.59900426864624, 'val_loss': 1.6961980465711173, 'val_ppl': 5.453175211124889, 'lr': 1.25}, {'epoch': 41, 'time': 499.00581073760986, 'val_loss': 1.673944449828843, 'val_ppl': 5.333162756099444, 'lr': 1.25}, {'epoch': 42, 'time': 499.14237689971924, 'val_loss': 1.6679496752205543, 'val_ppl': 5.301287286016513, 'lr': 1.25}, {'epoch': 44, 'time': 498.7149875164032, 'val_loss': 1.5260574812808279, 'val_ppl': 4.600005417817772, 'lr': 0.3125}, {'epoch': 45, 'time': 495.22376227378845, 'val_loss': 1.5057726746898585, 'val_ppl': 4.507635220693019, 'lr': 0.3125}, {'epoch': 46, 'time': 493.4229655265808, 'val_loss': 1.4977866539712679, 'val_ppl': 4.471780511212303, 'lr': 0.3125}, {'epoch': 47, 'time': 494.35696506500244, 'val_loss': 1.4940398597717286, 'val_ppl': 4.455057019236988, 'lr': 0.3125}, {'epoch': 48, 'time': 493.5741765499115, 'val_loss': 1.4745827446953725, 'val_ppl': 4.369212315516881, 'lr': 0.3125}, {'epoch': 50, 'time': 493.361328125, 'val_loss': 1.4451493263244628, 'val_ppl': 4.242485610305345, 'lr': 0.078125}]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "gihMJpjc5YXc",
        "colab_type": "code",
        "outputId": "69099d18-eceb-43fc-c205-0b59def93b9f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "Dictionary.predict('take major steps N years ago', test_model = test_model, corpus=corpus)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'major steps N years ago <eos>'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "metadata": {
        "id": "TSHh8MPP7xo2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Others"
      ]
    },
    {
      "metadata": {
        "id": "Zw-6SgZxhQ3G",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Dimensionality Analysis\n",
        "\n",
        "I used the implementation above to go through all the different dimensions. Maybe this helps you understanding how to implement the RNN / LSTM layer.\n",
        "\n",
        "I also tried looking at: https://www.quora.com/In-LSTM-how-do-you-figure-out-what-size-the-weights-are-supposed-to-be\n",
        "\n",
        "Also, look here: https://github.com/pytorch/pytorch/blob/v0.3.0/torch/nn/_functions/rnn.py\n",
        "\n",
        "### Some variables\n",
        "\n",
        "* bptt = \"backpropagation through time\", but here used as the sequence length that we feed at once. It's given in dim=0 aling the 'seq_len' dimension in the LSTM given in the original code. Given above to be 35.\n",
        "* bsz = \"batch size\", number of sequences looked at at once, in our case set to 20 in dim=1\n",
        "* ntokens = len(vocab), total number of different tokens in the data\n",
        "* len(text) = total number of tokens of the whole text\n",
        "* nhid = number of values in hidden layer\n",
        "* emsize = embedding size\n",
        "\n",
        "### Step by step\n",
        "\n",
        "1. in **corpus** only the index of every word is kept, so every word goes from dimension *ntokens* to a scalar value:   \n",
        "      dim token: [ntokens] -> [1]\n",
        "\n",
        "2. after **batchify(training_data)** , dividing the total text by the batch size and having *batch_size* many sequences:  \n",
        "      dim: [len(text)] -> [len(text) / bsz, bsz]\n",
        "\n",
        "3. after **get_batch(data, i)** get on sequence of size bptt for every batch:   \n",
        "    dim data: [len(text) / bsz,  bsz] -> [bptt, bsz]  \n",
        "    dim target: [len(text)] -> [bptt * bsz]\n",
        "    \n",
        "4. In the Net  \n",
        "    1. Input dim: [bptt, bsz]\n",
        "    2. hidden layer dim: (ht-1, ht): ([1, bsz, nhid], [1, bsz, nhid])\n",
        "    3. embedding layer dim: [bptt, bsz, emsize]\n",
        "    4. lstm layer dim: [bptt, bsz, emsize], hidden layer"
      ]
    },
    {
      "metadata": {
        "id": "BKqcRnmQXCko",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## TODO\n",
        "\n",
        "1. Implement LSTM cell (done)\n",
        "2. Bias term\n",
        "3. Think about initialization (done, used AWD paper settings)\n",
        "4. Multilayer (done)\n",
        "5. Bi directional \n",
        "6. Prediction module (done)\n",
        "7. Optimization from AWD paper (both orignal and Custom LSTM)\n",
        "  - DropConnect (for recurring connections)\n",
        "  - Variational dropout (all other) \n",
        "  - NT-ASGD\n",
        "  - Varying BPTT length (done)\n",
        "  - Embedding dropout (done)\n",
        "  - Weight tying (done)\n",
        "  - L2 regularization \n",
        "8. Convolutional Net\n",
        "\n",
        "  \n",
        "  \n",
        "\n",
        "\n",
        "\n",
        " "
      ]
    },
    {
      "metadata": {
        "id": "_EUOt3-C74lY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}